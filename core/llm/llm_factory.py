"""
AmyAlmond Project - core/llm/llm_factory.py

Open Source Repository: https://github.com/shuakami/amyalmond_bot
Developer: Shuakami <3 LuoXiaoHei
Copyright (c) 2024 Amyalmond_bot. All rights reserved.
Version: 1.3.0 (Stable_923001)

llm_factory.py - LLM å·¥å‚ç±»ï¼Œç”¨äºæ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºç›¸åº”çš„ LLM å®¢æˆ·ç«¯ã€‚
"""

from core.llm.llm_client import LLMClient
from core.llm.plugins.aliyun_client import AliyunClient
from core.llm.plugins.anthropic_client import AnthropicClient
from core.llm.plugins.azure_client import AzureClient
from core.llm.plugins.chatglm_client import ChatGLMClient
from core.llm.plugins.google_client import GoogleClient
from core.llm.plugins.openai_client import OpenAIClient
from config import test_config
from core.utils.logger import get_logger

_log = get_logger()


class LLMFactory:
    """
    LLM å·¥å‚ç±»ï¼Œç”¨äºæ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºç›¸åº”çš„ LLM å®¢æˆ·ç«¯ã€‚
    """

    def create_llm_client(self) -> LLMClient:
        """
        æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»º LLM å®¢æˆ·ç«¯ã€‚

        Returns:
            LLMClient: LLM å®¢æˆ·ç«¯å®ä¾‹ã€‚
        """

        # å¦‚æœç”¨æˆ·æ²¡æœ‰è®¾ç½®ï¼Œè­¦å‘Šä¸€ä¸‹
        if not test_config.get("llm_provider"):
            _log.warning("ğŸ”¥ä½ æ²¡æœ‰åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® LLM æä¾›å•†ï¼Œå·²ä½¿ç”¨é»˜è®¤ OpenAIã€‚ğŸ”¥")

        # æ‰“å°å½“å‰LLM
        _log.info(f"ğŸ”¥å½“å‰LLMå‚å•†ï¼š {test_config.get('llm_provider', 'openai')}ğŸ”¥")

        llm_provider = test_config.get("llm_provider", "openai").lower()  # è½¬æ¢ä¸ºå°å†™å­—æ¯

        # å®šä¹‰ä¸€éƒ¨å­—å…¸æ¥å­˜å‚¨æä¾›å•†å’Œå¯¹åº”çš„é…ç½®é¡¹
        provider_configs = {
            "openai": ["openai_secret", "openai_model", "openai_api_url"],
            "azure": ["azure_secret", "azure_model", "azure_api_url"],
            "google": ["google_api_key", "google_model", "google_api_url"],
            "anthropic": ["anthropic_secret", "anthropic_model", "anthropic_api_url"],
            "aliyun": ["aliyun_secret", "aliyun_model", "aliyun_api_url"],
            "chatglm": ["chatglm_secret", "chatglm_model", "chatglm_api_url"],
            "free_chatgpt_api": ["free_chatgpt_api_secret", "free_chatgpt_api_model", "free_chatgpt_api_url"]
        }

        # æ£€æŸ¥é…ç½®é¡¹æ˜¯å¦å­˜åœ¨
        if llm_provider in provider_configs:
            required_configs = provider_configs[llm_provider]
            missing_configs = [config for config in required_configs if not test_config.get(config)]
            if missing_configs:
                _log.error(f"ğŸ”¥ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¡«å†™ {llm_provider} çš„ç›¸å…³é…ç½®é¡¹ï¼š{'ã€'.join(missing_configs)} ğŸ”¥")
                # ä¸­æ–­ç¨‹åºæ‰§è¡Œ
                raise SystemExit(1)

        llm_provider = test_config.get("llm_provider", "openai")
        if llm_provider == "openai":
            return OpenAIClient(test_config.get("openai_secret"),
                                test_config.get("openai_model"),
                                test_config.get("openai_api_url"))
        elif llm_provider == "azure":
            return AzureClient(test_config.get("azure_secret"),
                               test_config.get("azure_model"),
                               test_config.get("azure_api_url"))
        elif llm_provider == "google":
            return GoogleClient(test_config.get("google_api_key"),
                                test_config.get("google_model"),
                                test_config.get("google_api_url"))
        elif llm_provider == "anthropic":
            return AnthropicClient(test_config.get("anthropic_secret"),
                                   test_config.get("anthropic_model"),
                                   test_config.get("anthropic_api_url"))
        elif llm_provider == "aliyun":
            return AliyunClient(test_config.get("aliyun_secret"),
                                test_config.get("aliyun_model"),
                                test_config.get("aliyun_api_url"))
        elif llm_provider == "chatglm":
            return ChatGLMClient(test_config.get("chatglm_secret"),
                                 test_config.get("chatglm_model"),
                                 test_config.get("chatglm_api_url"))
        else:
            raise ValueError(f"Unsupported LLM provider: {llm_provider}")
